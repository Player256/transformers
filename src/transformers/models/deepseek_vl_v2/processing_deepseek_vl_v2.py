#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/deepseek_vl_v2/modular_deepseek_vl_v2.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_deepseek_vl_v2.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

import torch
from PIL import Image, ImageOps
from torch.nn.utils.rnn import pad_sequence

from transformers.image_processing_utils import BatchFeature

from ...processing_utils import ProcessorMixin


class DeepseekVLV2Processor(ProcessorMixin):
    """
    Transformers-compatible processor for Deepseek VL v2.
    Inherits from DeepseekVLProcessor but overrides the behavior to:
      - handle tiling and cropping (global + local views)
      - expand <image> placeholders into the right number of tokens
      - build `images_seq_mask`, `images_spatial_crop`, and `num_image_tokens`.
    """

    attributes = ["image_processor", "tokenizer"]
    valid_kwargs = ["chat_template", "num_image_tokens"]
    image_processor_class = "AutoImageProcessor"
    tokenizer_class = "AutoTokenizer"

    def __init__(
        self,
        *args,
        candidate_resolutions: list[tuple[int, int]] = ((384, 384),),
        patch_size: int = 14,
        downsample_ratio: int = 2,
        **kwargs,
    ):
        super().__init__(image_processor, tokenizer, chat_template=chat_template)
        self.image_token = tokenizer.image_token
        self.num_image_tokens = num_image_tokens

        self.candidate_resolutions = candidate_resolutions
        self.image_size = candidate_resolutions[0][0]
        self.patch_size = patch_size
        self.downsample_ratio = downsample_ratio

    def __call__(self, text=None, images=None, return_tensors="pt", **kwargs) -> BatchFeature:
        """
        Override __call__ to return both text and image features.
        """
        if text is None and images is None:
            raise ValueError("You must specify either text or images.")

        if isinstance(text, str):
            text = [text]
        if images is None:
            images = [None] * len(text)

        (
            all_input_ids,
            all_attention_mask,
            all_images,
            all_masks,
            all_spatial,
            all_num_tokens,
        ) = ([], [], [], [], [], [])

        for t, im in zip(text, images):
            if im is None:
                enc = self.tokenizer(t, return_tensors=return_tensors)
                all_input_ids.append(enc["input_ids"].squeeze(0))
                all_attention_mask.append(enc["attention_mask"].squeeze(0))
                all_images.append(torch.zeros(1, 3, self.image_size, self.image_size))
                all_masks.append(torch.zeros_like(enc["input_ids"], dtype=torch.bool).squeeze(0))
                all_spatial.append(torch.zeros((1, 2), dtype=torch.long))
                all_num_tokens.append([0])
            else:
                ids, ims, mask, spatial, num_tokens = self.tokenize_with_images(t, [im])
                all_input_ids.append(torch.tensor(ids))
                all_attention_mask.append(torch.ones(len(ids), dtype=torch.long))
                all_images.append(torch.stack(ims))
                all_masks.append(torch.tensor(mask, dtype=torch.bool))
                all_spatial.append(torch.tensor(spatial, dtype=torch.long))
                all_num_tokens.append(num_tokens)

        # pad sequences
        input_ids = pad_sequence(all_input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)
        attention_mask = pad_sequence(all_attention_mask, batch_first=True, padding_value=0)
        images_seq_mask = pad_sequence(all_masks, batch_first=True, padding_value=0)

        return BatchFeature(
            data=dict(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=torch.nested.to_padded_tensor(torch.nested.nested_tensor(all_images), 0.0),
                images_seq_mask=images_seq_mask,
                images_spatial_crop=all_spatial,
                num_image_tokens=all_num_tokens,
            )
        )

    def batch_decode(self, *args, **kwargs):
        """
        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please
        refer to the docstring of this method for more information.
        """
        return self.tokenizer.batch_decode(*args, **kwargs)

    def decode(self, *args, **kwargs):
        """
        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to
        the docstring of this method for more information.
        """
        return self.tokenizer.decode(*args, **kwargs)

    @property
    def model_input_names(self):
        tokenizer_input_names = self.tokenizer.model_input_names
        image_processor_input_names = self.image_processor.model_input_names
        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))

    def tokenize_with_images(self, text: str, images: list[Image.Image]):
        """
        Tokenize text with <image> placeholders expanded into correct number of image tokens.
        Also preprocess images into tiles + global view.
        """
        assert text.count(self.image_token) == len(images), (
            f"Mismatched <image> count ({text.count(self.image_token)}) vs {len(images)} images."
        )

        (
            tokenized_str,
            images_list,
            images_seq_mask,
            images_spatial_crop,
            num_image_tokens,
        ) = ([], [], [], [], [])
        splits = text.split(self.image_token)

        for split_text, image in zip(splits, images):
            # text part
            token_ids = self.tokenizer.encode(split_text, add_special_tokens=False)
            tokenized_str.extend(token_ids)
            images_seq_mask.extend([False] * len(token_ids))

            # preprocess image: global + local views
            best_w, best_h = self.candidate_resolutions[0]  # TODO: select best like official code
            global_view = ImageOps.pad(image, (self.image_size, self.image_size))
            images_list.append(self.image_processor(global_view)["pixel_values"])

            local_view = ImageOps.pad(image, (best_w, best_h))
            for i in range(0, best_h, self.image_size):
                for j in range(0, best_w, self.image_size):
                    crop = local_view.crop((j, i, j + self.image_size, i + self.image_size))
                    images_list.append(self.image_processor(crop)["pixel_values"])

            # track tiling shape
            num_w_tiles, num_h_tiles = (
                best_w // self.image_size,
                best_h // self.image_size,
            )
            images_spatial_crop.append([num_w_tiles, num_h_tiles])

            # expand <image> token â†’ num_image_tokens
            h = w = (self.image_size // self.patch_size) // self.downsample_ratio
            image_token_ids = [self.tokenizer.convert_tokens_to_ids(self.image_token)] * (
                h * (w + 1) + 1 + (num_h_tiles * h) * (num_w_tiles * w + 1)
            )
            tokenized_str.extend(image_token_ids)
            images_seq_mask.extend([True] * len(image_token_ids))
            num_image_tokens.append(len(image_token_ids))

        # last text after final image
        last_split = splits[-1]
        if last_split:
            token_ids = self.tokenizer.encode(last_split, add_special_tokens=False)
            tokenized_str.extend(token_ids)
            images_seq_mask.extend([False] * len(token_ids))

        return (
            tokenized_str,
            images_list,
            images_seq_mask,
            images_spatial_crop,
            num_image_tokens,
        )
